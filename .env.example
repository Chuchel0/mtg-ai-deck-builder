# LLM Provider Configuration
# Options: 'ollama', 'huggingface_inference', 'openai_compatible'
LLM_PROVIDER="ollama"

# Base URL for local Ollama instance (if used)
OLLAMA_BASE_URL="http://localhost:11434"

# API Key for other providers (if used)
# HF_API_KEY=""
# OPENAI_API_KEY=""
